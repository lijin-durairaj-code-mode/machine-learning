{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrf8FfU2xWAnYVK1uabkwZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lijin-durairaj-code-mode/machine-learning/blob/main/movie_recommender_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nAZjAAYNq5d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import re\n",
        "\n",
        "#nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "#sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#config\n",
        "pd.set_option('display.max_rows',None)\n",
        "pd.set_option('display.max_columns',None)\n",
        "pd.set_option('display.max_colwidth', 1000000)\n",
        "\n",
        "#objects\n",
        "stemming=PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class movie_recommender:\n",
        "\n",
        "    def pull_data(self):\n",
        "        movies_data=pd.read_csv(r'tmdb_5000_movie_rating\\tmdb_5000_movies.csv')\n",
        "        credits_data=pd.read_csv(r'tmdb_5000_movie_rating\\tmdb_5000_credits.csv')\n",
        "        return movies_data,credits_data\n",
        "\n",
        "    def extract_crew(self,_crew):\n",
        "        crew_name=[]\n",
        "        for _a in json.loads(_crew):\n",
        "            if _a['department'] in ['Directing','Editing','Camera']:\n",
        "                crew_name.append(_a['name'])\n",
        "        return ', '.join(crew_name)\n",
        "\n",
        "    def get_keywords(self,_list):\n",
        "        keywords=[]\n",
        "        for _genres in json.loads(_list):\n",
        "            keywords.append(_genres['name'])\n",
        "        return ', '.join(keywords)\n",
        "\n",
        "    def pipeline_action(self,movies_data,credits_data):\n",
        "        _merged_data=movies_data.merge(credits_data,left_on='id',right_on='movie_id',suffixes=('_movies_dup', '_credits_dup'))\n",
        "        _merged_data['genres_extracted_word']=_merged_data['genres'].apply(lambda x:self.get_keywords(x))\n",
        "        _merged_data['keywords_extracted_word']=_merged_data['keywords'].apply(lambda x:self.get_keywords(x))\n",
        "        _merged_data['extracted_crew']=_merged_data['crew'].apply(lambda x:self.extract_crew(x))\n",
        "        _merged_data['keywords']=_merged_data.apply(lambda x:(str(x['overview'])+str(x['tagline'])+str(x['title_movies_dup'])+str(x['vote_average'])+str(x['extracted_crew'])+str(x['genres_extracted_word'])+str(x['keywords_extracted_word'])),axis=1)\n",
        "        _merged_data.drop(['budget', 'homepage',   'original_language',\n",
        "           'original_title', 'overview', 'popularity', 'production_companies',\n",
        "           'production_countries', 'release_date', 'revenue', 'runtime',\n",
        "           'spoken_languages', 'status', 'tagline',\n",
        "           'vote_average', 'vote_count', 'movie_id', 'title_credits_dup', 'cast',\n",
        "           'crew', 'genres_extracted_word', 'keywords_extracted_word',\n",
        "           'extracted_crew'],axis=1,inplace=True)\n",
        "        return _merged_data\n",
        "\n",
        "    def edit_sentences(self,_text):\n",
        "        #remove stopword\n",
        "        _text_ouput=[_word for _word in word_tokenize(_text) if _word not in english_stopwords]\n",
        "        #remove puntutation\n",
        "        _text_ouput=[word for word in _text_ouput if word not in string.punctuation]\n",
        "        #stemming\n",
        "        _text_ouput=[stemming.stem(word) for word in _text_ouput]\n",
        "        #lowercase\n",
        "        _text_ouput=[word.lower() for word in _text_ouput]\n",
        "        return ' '.join(_text_ouput)\n",
        "\n",
        "    def apply_vectorization(self,_corpus):\n",
        "        count_vectorizer=CountVectorizer(stop_words='english',lowercase=True,min_df=5)\n",
        "        transformed_data=count_vectorizer.fit_transform(_corpus)\n",
        "        return transformed_data,count_vectorizer\n",
        "\n",
        "    def main(self):\n",
        "        movies_data,credits_data=self.pull_data()\n",
        "        _data=self.pipeline_action(movies_data,credits_data)\n",
        "        _data['edited_words']=_data['keywords'].apply(lambda x:self.edit_sentences(x))\n",
        "        transformed_data,count_vectorizer=self.apply_vectorization(_data['edited_words'])\n",
        "        _cosine_similarity=cosine_similarity(transformed_data)\n",
        "        return _cosine_similarity,count_vectorizer,_data\n",
        "\n",
        "\n",
        "\n",
        "movies=movie_recommender()\n",
        "# _cosine_similarity,count_vectorizer,_data=movies.main()"
      ],
      "metadata": {
        "id": "Wt_mom_W--8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_movies(_movie,_cosine_similarity):\n",
        "    movie_index=_data[_data['title_movies_dup']==_movie].index[0]\n",
        "    _sorted_index=sorted(list(enumerate(_cosine_similarity[movie_index])),key=lambda x:x[1],reverse=True)[1:6]\n",
        "    _genres_lst=[]\n",
        "    for _i in _sorted_index:\n",
        "        for _genres in json.loads(_data.iloc[_i[0],:]['genres']):\n",
        "            _genres_lst.append(_genres['name'])\n",
        "        print('movie name - {0} ---- genres {1}'.format(_data.iloc[_i[0],:]['title_movies_dup'],', '.join(_genres_lst)))\n"
      ],
      "metadata": {
        "id": "aCt1by_u_D3J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}